# -*- coding: utf-8 -*-
"""PRML_Minor_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13kxnMPiWRU4y0mROCMN3QlKuVe_NMTO2

## **Importing libraries**
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import GridSearchCV
import plotly.express as exp
import plotly.express as px


# Loading the dataset
data = pd.read_csv('/content/Country-data.csv')

"""## **Exploratory Data Analysis** """

# Displaying the first five rows of the dataset
data.head()

# Displaying the shape of the dataset
data.shape

"""## **Data Preprocessing and Visualisation**"""

# Converting exports, health, and imports columns from percentages to actual values
data['exports'] = data['exports']*data['gdpp']/100
data['health'] = data['health']*data['gdpp']/100
data['imports'] = data['imports']*data['gdpp']/100

# Displaying the summary of the dataset
data.describe()

# Checking for missing values
data.isnull().sum()

# Visualizing the correlation between the features
plt.figure(figsize=(5,3))
sns.heatmap(data.corr(), annot=True)
plt.show()

plot = px.choropleth(data,
                    locationmode='country names',
                    locations='country',
                    color='gdpp',
                    title='Countries by gdpp'
                   )
plot.show()

plot = px.choropleth(data,
                    locationmode='country names',
                    locations='country',
                    color='income',
                    title='Countries by income'
                   )
plot.show()

fig = px.choropleth(data,
                    locationmode='country names',
                    locations='country',
                    color='health',
                    title='Countries by health'
                   )
fig.show()

exp.scatter(data_frame=data,x = 'child_mort',y = 'health',color='country')

exp.scatter(data_frame=data,x = 'child_mort',y = 'income',color='country')

exp.scatter(data_frame=data,x = 'child_mort',y = 'gdpp',color='country')

# Removing the unnecessary columns
data = data.drop(['country'], axis=1)

sns.set_style('darkgrid')
plt.figure(figsize=(15, 15))
for i, column in enumerate(data.columns):
    plt.subplot(3, 3, i+1)
    sns.histplot(data[column], kde=True)
plt.show()

# Normalizing the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Creating a dataframe from the scaled data
scaled_df = pd.DataFrame(scaled_data, columns=data.columns)
scaled_df.head()

"""## **Dimensionality Reduction using PCA**"""

import numpy as np
from sklearn.decomposition import PCA

# instantiate a PCA object
pca = PCA()

# fit the PCA model to the data
pca.fit(data)

# calculate the explained variance ratio and the cumulative explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# set the desired percentage of variance to retain
percentage_to_retain = 0.95

# find the number of components needed to retain the desired percentage of variance
n_components = np.argmax(cumulative_variance_ratio >= percentage_to_retain) + 1

# print the number of components needed
print("Number of components needed to retain {}% of variance: {}".format(percentage_to_retain*100, n_components))

# create a line plot of the explained variance ratio 
plt.plot(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, label='Explained Variance Ratio')
plt.xlabel('Number of Components')
plt.ylabel('Variance Ratio')
plt.legend()
plt.show()

# create a line plot of the cumulative explained variance ratio
plt.plot(range(1, len(cumulative_variance_ratio)+1), cumulative_variance_ratio, label='Cumulative Explained Variance Ratio')
plt.xlabel('Number of Components')
plt.ylabel('Variance Ratio')
plt.legend()
plt.show()

# Performing PCA
pca = PCA(n_components=2)
data_pca = pca.fit_transform(scaled_data)

# Creating a dataframe from the PCA data
pca_df = pd.DataFrame(data_pca)
pca_df.head()

"""## **Hyperparameter Tuning**"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Creating a dataframe for the PCA results
pca_df = pd.DataFrame(data=data_pca, columns=['PCA1', 'PCA2'])
pca_df.head()


# perform k-means clustering with different number of clusters
k_values = range(2, 11)
k_scores = []
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_pca)
    labels = kmeans.labels_
    score = silhouette_score(data_pca, labels)
    k_scores.append(score)

# plot the silhouette scores for different number of clusters
plt.plot(k_values, k_scores, 'o-')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.title('K-means clustering')
plt.show()

# perform hierarchical clustering with different number of clusters
h_values = range(2, 11)
h_scores = []
for h in h_values:
    hierarchical = AgglomerativeClustering(n_clusters=h)
    hierarchical.fit(data_pca)
    labels = hierarchical.labels_
    score = silhouette_score(data_pca, labels)
    h_scores.append(score)

# plot the silhouette scores for different number of clusters
plt.plot(h_values, h_scores, 'o-')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.title('Hierarchical clustering')
plt.show()

# perform DBSCAN clustering with different values of eps
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# define the hyperparameters to tune
eps_values = [0.1, 0.5, 1.0]
min_samples_values = [2,3, 5, 10]

for eps in eps_values:
    for min_samples in min_samples_values:
        try:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            dbscan.fit(data_pca)
            labels = dbscan.labels_
            if len(set(labels)) > 1:
                score = silhouette_score(data_pca, labels)
                print(f"DBSCAN with eps={eps}, min_samples={min_samples}: {score}")
            else:
                print(f"Not enough clusters with eps={eps}, min_samples={min_samples}")
        except Exception as e:
            print(f"Error with eps={eps}, min_samples={min_samples}: {str(e)}")

"""## **KMeans Clustering**"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans.fit(data_pca)

"""## **Hierarchical clustering**"""

from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# Perform hierarchical clustering
hierarchical = AgglomerativeClustering(n_clusters=4, linkage='ward')
hierarchical.fit(data_pca)

# Compute the linkage matrix
Z = linkage(scaled_data, method='ward')

# Visualize the dendrogram to determine the optimal number of clusters
plt.figure(figsize=(25, 10))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Countries')
plt.ylabel('Distance')
dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)
plt.show()

"""## **DBSCAN Clustering**"""

from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=1, min_samples=3)
dbscan.fit(data_pca)

"""## **Ensemble clustering**"""

# Ensemble clustering
ensemble_clusters = []
for i in range(len(data_pca)):
    cluster = []
    cluster.append(hierarchical.labels_[i])
    cluster.append(dbscan.labels_[i])
    cluster.append(kmeans.labels_[i])
    ensemble_clusters.append(max(set(cluster), key=cluster.count))

"""## **Finding Best Algorithm using different scoring techniques**

### **Silhouette score**
"""

from sklearn.metrics import silhouette_score

# Calculate silhouette score for each clustering algorithm
print('Silhouette Score for K-Means Clustering:', silhouette_score(data_pca, kmeans.labels_))
print('Silhouette Score for Hierarchical Clustering:', silhouette_score(data_pca, hierarchical.labels_))
print('Silhouette Score for DBSCAN Clustering:', silhouette_score(data_pca, dbscan.labels_))
print("Silhouette score for ensemble clustering: ", silhouette_score(data_pca, ensemble_clusters))

"""### **Calinski_harabasz_score**"""

from sklearn.metrics import calinski_harabasz_score
print('Calinski-Harabasz Index for KMeans Clustering:', calinski_harabasz_score(data_pca, kmeans.labels_))
print('Calinski-Harabasz Index for Hierarchical Clustering:', calinski_harabasz_score(data_pca, hierarchical.labels_))
print('Calinski-Harabasz Index for DBSCAN Clustering:', calinski_harabasz_score(data_pca, dbscan.labels_))
print('Calinski-Harabasz Index for Ensemble Clustering:', calinski_harabasz_score(data_pca, ensemble_clusters))

"""### **Davies-Bouldin Index**"""

from sklearn.metrics import davies_bouldin_score
print('Davies-Bouldin Index for KMeans Clustering:', davies_bouldin_score(data_pca, kmeans.labels_))
print('Davies-Bouldin Index for Hierarchical Clustering:', davies_bouldin_score(data_pca, hierarchical.labels_))
print('Davies-Bouldin Index for DBSCAN Clustering:', davies_bouldin_score(data_pca, dbscan.labels_))
print('Davies-Bouldin Index for Ensemble Clustering:', davies_bouldin_score(data_pca, ensemble_clusters))

"""## **Visualisation of Clusters**"""

plt.scatter(data_pca[:,0], data_pca[:,1], c=kmeans.labels_, cmap='viridis')
plt.title('KMeans Clustering')
plt.show()

plt.scatter(data_pca[:,0], data_pca[:,1], c=hierarchical.labels_, cmap='viridis')
plt.title('Hierarchical Clustering')
plt.show()

plt.scatter(data_pca[:,0], data_pca[:,1], c=dbscan.labels_, cmap='viridis')
plt.title('DBSCAN Clustering')
plt.show()

plt.scatter(data_pca[:,0], data_pca[:,1], c=ensemble_clusters, cmap='viridis')
plt.title('Ensemble Clustering')
plt.show()

"""## **Clusters of KMeans Algorithm**"""

# read data from CSV file
country_data = pd.read_csv('/content/Country-data.csv')

# add country column back to data DataFrame
data['country'] = country_data['country']

# Add the cluster labels to the DataFrame
data['cluster'] = kmeans.labels_

data[kmeans.labels_==0]

data[kmeans.labels_==1]

data[kmeans.labels_==2]

data[kmeans.labels_==3]

# Number of countries in each cluster
import numpy as np
np.unique(kmeans.labels_, return_counts=True)

"""## **Aid amount distribution among clusters**"""

# Calculate total need (i.e., GDP + health + social factor) for each cluster
cluster_totals = data.groupby('cluster').sum()
cluster_need = cluster_totals['gdpp'] + cluster_totals['health'] + cluster_totals['income']

# Calculate proportion of total aid needed for each cluster
cluster_prop = cluster_need / cluster_need.sum()

# Calculate total aid for all clusters
total_aid = 10000000

# Calculate aid amount for each cluster based on available funding and proportion of total need
cluster_aid_amount = total_aid * cluster_prop

# Print the total aid amount for each cluster
for i in range(len(cluster_aid_amount)):
    print('Cluster', i, ':', round(cluster_aid_amount[i], 2))

"""## **Calculating aid by giving each weights**"""

import pandas as pd

# Define weights for different factors
weights = {
    'gdpp': 0.4,
    'life_expec': 0.2,
    'total_fer': 0.1,
    'income': 0.15,
    'inflation': 0.05,
    'health': 0.1,
}

# Calculate score for each country
scores = []
for i in range(len(data)):
    score = 0
    for factor, weight in weights.items():
        # Apply inverse weighting to GDP per capita and income factors
        if factor in ['gdpp', 'income']:
            score += weight / data.loc[i, factor]
        else:
            score += weight * data.loc[i, factor]
    scores.append(score)

# Calculate aid needed for each country proportional to its score
total_aid = 10000000
aid_amounts = (total_aid * pd.Series(scores)) / sum(scores)

# Print aid amount for each country
for i in range(len(data)):
    print(data.loc[i, 'country'], ':', round(aid_amounts[i], 2))

"""## **Visualisation of distribution of aid among clusters**"""

import matplotlib.pyplot as plt

# Calculate total aid for each cluster
cluster_aid = [cluster_aid_amount[0],cluster_aid_amount[1],cluster_aid_amount[2],cluster_aid_amount[3]]

# Plot stacked bar chart
plt.bar(range(len(cluster_aid)), cluster_aid, label='Aid', color='b')
plt.bar(range(len(cluster_need)), -cluster_need, label='Need', color='r')
plt.xticks(range(len(cluster_aid)), ['Cluster 0', 'Cluster 1', 'Cluster 2', 'Cluster 3'])
plt.xlabel('Cluster')
plt.ylabel('Amount ($)')
plt.title('Distribution of Aid by Cluster')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Create a scatter plot to visualize aid distribution for each country
colors = ['red', 'blue', 'green', 'yellow']
cluster_colors = [colors[i] for i in kmeans.labels_]
plt.scatter(x=data['gdpp'], y=data['health'], c=cluster_colors, s=data['income']*0.001, alpha=0.5)
plt.title('Distribution of Aid by Country')
plt.xlabel('GDP Per Capita')
plt.ylabel('Health Factor')
plt.show()